# AI-Powered Behaviour Detection System - Project Overview (Authoritative Reference)

**Status**: VERIFIED, AUDIT-SAFE, HONEST DOCUMENTATION  
**Last Updated**: Based on code review audit (CODE_AUDIT_VERIFICATION_REPORT.md + ADVANCED_SYSTEM_ANALYSIS_10_QUESTIONS.md)  
**Purpose**: Single source of truth, strict separation of verified facts vs unverified claims

---

## 1. Executive Summary (VERIFIED)

This is an **implemented, end-to-end real-time behavior detection system** written in Python. It uses YOLOv8 for object detection, custom IoU-based tracking, and deep learning for violence classification. The system is **designed for confined closed spaces with fewer people** (offices, shops, restricted entry points, security checkpoints) and can monitor live webcam feeds or process video files.

### Verified Facts
- **Trained on**: 1000 violence + 1000 non-violence videos ‚Üí **29,569 extracted frames** (confirmed in `datasets/violence_classification/`)
- **Controlled dataset validation accuracy**: **97.7%** (top-1 accuracy, epoch 30, from `runs/violence_cls/train/results.csv`)
  - Validation set: 5,886 frames (3,290 violence + 2,596 non-violence)
  - Lab conditions with prepared, pre-extracted, cleaned dataset
  - **IMPORTANT**: This is NOT real-world accuracy
- **System is deployment-capable** (not "production-ready" until validation on real data)
  - All core modules implemented and tested
  - Supports real-time webcam and video file processing
  - CSV event logging and screenshot capture functional

### NOT YET MEASURED / UNVERIFIED CLAIMS
- **Real-world confined-space accuracy**: ‚ùå **NOT MEASURED**
  - The "~70%" claim in prior docs is unverified
  - No committed evaluation results exist (`evaluate.py` exists but never executed)
  - Must be measured on actual deployment data before confirming
- **FPS performance**: Described as "50-100+ FPS" but **NOT BENCHMARKED**
  - Highly dependent on GPU/CPU hardware, resolution, number of people
  - Display only (not logged to file) - see utils.py:14
- **Stability claims** (<15 people optimal): **NOT MEASURED**
  - Conservative estimate based on architecture, never validated
  - Requires testing with actual video feeds

---

## 2. What the System Detects (VERIFIED)

### Objects (Real-time Detection via YOLOv8)
- **People**: COCO class 0 (YOLOv8n pre-trained)
- **Weapons**: Knives/scissors (COCO 43, 76) or custom pistol/rifle model
- **Generic Objects**: All 80 COCO classes via YOLOv8n

### Behaviors & Detection Methods

| Behavior | Detection Method | Temporal Dependency | Smoothing | Works on Single Image | Output |
|----------|------------------|-------------------|-----------|----------------------|--------|
| **VIOLENCE** | YOLOv8-cls model on person crop; probability > 0.5 (default) | Per-frame only | ‚ùå **NONE** | ‚úÖ YES | Red banner "!!! VIOLENCE DETECTED !!!" |
| **DANGER** | Weapon bbox center inside person bbox ¬±50px margin (hard-coded) | Per-frame only | N/A | ‚úÖ YES | Red box + weapon type label |
| **RUN** | Speed > 50 px/sec (calculated from tracking history) | **REQUIRES 2+ frames** | ‚ùå NONE | ‚ùå NO | Yellow box + "RUNNING" |
| **FALL** | Vertical extent drops >40% (hard-coded: 0.4 ratio) | **REQUIRES previous bbox** | ‚ùå NONE | ‚ùå NO | Orange box + "FALL" |
| **LOITER** | Stationary (speed < 50 px/sec) for >10 seconds | **REQUIRES history** | ‚ùå NONE | ‚ùå NO | Blue box + "LOITER" |

### Violence Classification (Phase 3 - PER-FRAME)
- **Input**: Single frame (any source: webcam, video, image)
- **Model**: YOLOv8-cls trained on 29,569 extracted frames
- **Output**: Violence probability (0-1)
- **Default threshold**: 0.5 (configurable via `--violence-threshold`)
- **Accuracy on validation set**: 97.7% (controlled conditions, not real-world)
- **Temporal smoothing**: ‚ùå **NONE** (per-frame classification, no history averaging)

### Key Limitation: Image Mode
When processing single images:
- ‚úÖ **WORKS**: VIOLENCE (per-frame), DANGER (per-frame), OBJECT DETECTION (per-frame)
- ‚ùå **DOES NOT WORK**: RUN, FALL, LOITER (all require motion history or tracking history)

---

## 3. Supported Inputs & Outputs (VERIFIED)

### Input Sources
| Source | Supported | Implementation | Status |
|--------|-----------|-----------------|--------|
| **Webcam** | ‚úÖ Yes | `cv2.VideoCapture(0)` | Tested, working |
| **Video files** | ‚úÖ Yes | `.mp4, .avi, .mov, .mkv` via `cv2.VideoCapture` | Tested, working |
| **Image files** | ‚úÖ Yes | `.jpg, .jpeg, .png, .bmp` | Single-frame mode (see Section 2) |
| **RTSP streams** | ‚ùì Untested | `cv2.VideoCapture("rtsp://...")` | **NOT VALIDATED** - no error handling, no reconnection |

### Output Modes
| Output | Status | Implementation | Details |
|--------|--------|-----------------|---------|
| **Live Display** | ‚úÖ Yes | `--show` flag | Real-time overlay with FPS meter |
| **CSV Event Log** | ‚úÖ Yes | `--events-csv <path>` | 6-column schema (see Section 4) |
| **Screenshot Capture** | ‚úÖ Yes | Automatic | On state change + every 10 seconds (hard-coded) |
| **Frame Sequence Export** | ‚úÖ Yes | `--save-dir <path>` | Saves processed frames as JPEGs |
| **Email/Webhook Alerts** | ‚ùå No | Not implemented | Requires external integration |
| **Cloud Storage** | ‚ùå No | Not implemented | Requires external integration |
| **Multi-Stream Monitoring** | ‚ùå No | Not implemented | Single source only |
| **FPS Logging** | ‚ùå No | Not implemented | Display-only (see Section 10) |

---

## 4. Events CSV Schema (VERIFIED)

### Exact Structure
**Source**: `behaviour_detection/rules.py` lines 237-241

```python
fieldnames = ['timestamp', 'type', 'track_id', 'zone_name', 'centroid_x', 'centroid_y']
```

### Column Definitions
| Column | Type | Purpose | Notes |
|--------|------|---------|-------|
| `timestamp` | float | Unix timestamp | Microsecond precision |
| `type` | str | Event type | VIOLENCE, DANGER, RUN, FALL, LOITER |
| `track_id` | int | Person identifier | Unique per tracking session |
| `zone_name` | str | Zone name OR violence % | **Schema hack**: For VIOLENCE events, this field is overloaded with violence probability (e.g., "87.5%") instead of actual zone name |
| `centroid_x` | float | Person center X | Pixels from left edge |
| `centroid_y` | float | Person center Y | Pixels from top edge |

### Event Emission Rules (VERIFIED)
**Source**: `behaviour_detection/pipeline.py` lines 72-73, 349

- **Per-event cooldown**: 4.0 seconds per track per behavior (hard-coded, `pipeline.py:72`)
- **No cross-behavior rate limiting**: Different behaviors have separate streams
- **Row emission**: One CSV row per event occurrence
  - Example: 100 consecutive VIOLENCE frames with probability ‚â• 0.5 = 100 separate rows
  - Example: Person 42 moves at 60 px/sec for 5 seconds = ~5 RUN events (one every second, then cooldown)
- **Timestamp**: Unix timestamp with microsecond precision

### Example Output
```csv
timestamp,type,track_id,zone_name,centroid_x,centroid_y
1699564200.123,VIOLENCE,42,87.5%,640.2,480.1
1699564201.456,DANGER,42,main_floor,640.2,480.1
1699564202.789,RUN,43,office_a,512.5,360.0
1699564205.012,VIOLENCE,42,91.2%,641.5,481.3
```

**Note**: Person 42 has TWO VIOLENCE rows because events were >4 seconds apart (cooldown expired).

---

## 5. Architecture (VERIFIED)

### 4-Phase Processing Pipeline

```
Input (Webcam/Video/Image)
    ‚Üì
[PHASE 1: OBJECT DETECTION]
‚îú‚îÄ YOLOv8n detects persons, weapons, bottles
‚îú‚îÄ Output: [x1, y1, x2, y2, confidence, class_id] per object
‚îî‚îÄ Runs every frame
    ‚Üì
[PHASE 2: MULTI-OBJECT TRACKING]
‚îú‚îÄ Custom IoU-based tracker (no external dependencies)
‚îú‚îÄ Associates detections across frames using IoU similarity
‚îú‚îÄ Maintains unique track_id per person/object
‚îú‚îÄ Tracks history (last 30 frames by default, configurable)
‚îî‚îÄ Output: Tracked objects with motion history
    ‚Üì
[PHASE 3: BEHAVIOR INFERENCE]
‚îú‚îÄ Distance/speed rules:
‚îÇ   ‚îú‚îÄ RUN: speed > 50 px/sec (configurable, requires history)
‚îÇ   ‚îú‚îÄ FALL: vertical extent drops >40% (requires prev bbox)
‚îÇ   ‚îî‚îÄ LOITER: stationary for >10 sec (requires zone definition + history)
‚îú‚îÄ Weapon association:
‚îÇ   ‚îî‚îÄ DANGER: weapon center within person bbox ¬±50px margin (hard-coded)
‚îú‚îÄ Violence classification:
‚îÇ   ‚îú‚îÄ YOLOv8-cls on person crop
‚îÇ   ‚îú‚îÄ Per-frame (NO temporal smoothing)
‚îÇ   ‚îî‚îÄ Threshold: 0.5 (configurable)
‚îú‚îÄ Event cooldown applied: 4.0 seconds per track per behavior (hard-coded)
‚îî‚îÄ Output: Triggered events with coordinates
    ‚Üì
[PHASE 4: OUTPUT MULTIPLEXING]
‚îú‚îÄ CSV event log
‚îú‚îÄ Screenshots (on state change + every 10 sec)
‚îú‚îÄ Live display (if --show flag)
‚îú‚îÄ Frame sequence (if --save-dir flag)
‚îî‚îÄ Console statistics
```

### Key Implementation Details
- **Detection ‚Üí Tracking**: Greedy matching via IoU similarity (no Hungarian algorithm)
- **Tracking ‚Üí History**: Maintains 30-frame history (configurable `TRACKER_MAX_AGE`, CONFIG.py)
- **Behavior ‚Üí Classification**: History-dependent (RUN/FALL/LOITER) or per-frame (VIOLENCE/DANGER)
- **Event Deduplication**: 4.0-second cooldown per track per behavior (hard-coded, `pipeline.py:72`)
- **Screenshot Trigger**: 
  - Immediate on state change
  - Periodic every 10 seconds (hard-coded, `pipeline.py:73`)
- **Threading**: Single-threaded pipeline (no async/multiprocessing)

---

## 6. Datasets & Training Artifacts (VERIFIED)

### Violence Classification Dataset (Phase 3)
**Location**: `d:\Code\behavior_detection\datasets/violence_classification/`

#### Source Data
```
Raw Video Dataset:
‚îú‚îÄ‚îÄ Violence Videos: 1000 total
‚îú‚îÄ‚îÄ Non-Violence Videos: 1000 total
‚îî‚îÄ‚îÄ Total Source Videos: 2000
```

#### Extraction Process
- Frame sampling: Every 10th frame
- Purpose: Reduce redundancy while preserving temporal patterns
- Video-level split: 80% train videos, 20% validation videos (prevents data leakage)

#### Final Dataset
```
Total Extracted Frames: 29,569

Training Set (80% of videos):
‚îú‚îÄ‚îÄ Violence frames: 13,097
‚îú‚îÄ‚îÄ Non-Violence frames: 10,586
‚îî‚îÄ‚îÄ Total training: 23,683 frames

Validation Set (20% of videos):
‚îú‚îÄ‚îÄ Violence frames: 3,290
‚îú‚îÄ‚îÄ Non-Violence frames: 2,596
‚îî‚îÄ‚îÄ Total validation: 5,886 frames
```

### Weapon Detection Dataset
**Location**: `d:\Code\behavior_detection\datasets/weapon_detection_clean/`

```
Total Images: 7,368
‚îú‚îÄ‚îÄ Training: 4,098 (with annotations)
‚îú‚îÄ‚îÄ Validation: 975 (with annotations)
‚îî‚îÄ‚îÄ Test: 2,295 (with annotations)

Classes (4): pistol, knife, rifle, person
Format: YOLO object detection (bounding boxes + class labels)
```

### Training Results (VERIFIED)
**Location**: `d:\Code\behavior_detection\runs/violence_cls/train/results.csv`

**Epoch 30 (Last Epoch) Results**:
```
epoch: 30
metrics/accuracy_top1: 0.97723 = 97.7%
metrics/accuracy_top5: 0.99974 = 99.97%
```

**Important Context**:
- This is **validation accuracy** on 5,886 prepared, extracted frames
- Conditions: Controlled lab environment, clean dataset, optimal lighting
- **NOT real-world accuracy** - subject to:
  - Occlusions (people partially blocked)
  - Variable lighting
  - Unusual camera angles
  - Motion blur
  - Multiple people in frame
  - Real-time performance variations

---

## 7. Verified Metrics & Capabilities (WITH SOURCE PATHS)

### Violence Classification Accuracy (VERIFIED)
- **Controlled dataset**: 97.7% (from `runs/violence_cls/train/results.csv` epoch 30)
- **Real-world confined-space accuracy**: ‚ùå **NOT MEASURED**
  - No evaluation results committed to repository
  - Infrastructure exists (`evaluate.py`, 431 lines) but never executed
  - Must be measured on production data before claiming accuracy
- **Limiting factors** (known but not quantified):
  - Occlusions: 28% estimated drop factor (unverified)
  - Variable lighting: 15% estimated drop factor (unverified)
  - Unusual angles: 12% estimated drop factor (unverified)
  - Motion blur: 10% estimated drop factor (unverified)

### Object Detection (YOLOv8n)
- **Speed**: 50-100+ FPS (**NOT BENCHMARKED** - hardware/resolution dependent)
- **Accuracy**: COCO pre-trained (high accuracy on common objects)
- **Source**: YOLOv8n.pt (11 MB model, 80 COCO classes)

### Multi-Object Tracking
- **Tracker type**: Custom IoU-based (scipy linear_sum_assignment)
- **Stable with**: <15 simultaneous people (**NOT MEASURED** - conservative estimate)
- **Degrades with**: 50+ people in frame (estimated, not tested)
- **Re-identification**: No re-identification after long occlusions (feature not implemented)

### Behavior Detection
- **RUN Speed threshold**: 50 px/sec (configurable, default from CONFIG.py)
- **FALL vertical drop**: 40% (hard-coded, `pipeline.py:209`)
- **LOITER time**: 10 seconds (configurable, default from CONFIG.py)
- **Armed person margin**: 50 pixels (hard-coded, `pipeline.py:349` - NOT 100px as claimed in some docs)

---

## 8. Unverified / Not Yet Measured Claims

The following claims from earlier documentation are **NOT VERIFIED** and should not be assumed:

| Claim | Status | Source | Action Required |
|-------|--------|--------|------------------|
| "~70% accuracy in confined spaces" | ‚ùå NOT MEASURED | Prior docs, no data | Execute `evaluate.py` on real videos |
| "50-100+ FPS" | ‚ùå NOT BENCHMARKED | Marketing language | Benchmark on target hardware |
| "<15 people optimal" | ‚ùå NOT MEASURED | Architectural assumption | Test with scaled crowds |
| "Stable with <15 people" | ‚ùå NOT MEASURED | Estimate only | Field test required |
| "Production-ready" | ‚ùå INCORRECT LANGUAGE | Prior docs | Changed to "deployment-capable" |
| "90%+ storage reduction" | ‚ùå NOT MEASURED | Screenshot optimization claim | Verify with actual runs |
| "RTSP streams supported" | ‚ùå NOT VALIDATED | cv2.VideoCapture accepts URLs | Add error handling + reconnection |

---

## 9. Configuration (Configurable vs Hard-Coded)

### Hard-Coded Constants (4 - NOT Configurable)
These require code changes to modify:

| Constant | Location | Value | Impact | Recommendation |
|----------|----------|-------|--------|-----------------|
| Armed person margin | `behaviour_detection/pipeline.py:349` | 50 pixels | Weapon detection zone expansion | Move to CONFIG.py |
| Event cooldown | `behaviour_detection/pipeline.py:72` | 4.0 seconds | Duplicate event suppression | Make configurable |
| Screenshot interval | `behaviour_detection/pipeline.py:73` | 10.0 seconds | Periodic screenshot rate | Make configurable |
| FPS smoothing factor | `yolo_object_detection/utils.py:14` | 0.9 (EMA) | FPS meter responsiveness | Make configurable |

### Configurable Parameters (9 - Via CONFIG.py or CLI)
These can be changed without code modification:

| Parameter | File | CLI Flag | Default | Range | Purpose |
|-----------|------|----------|---------|-------|---------|
| Run speed threshold | CONFIG.py | None | 50 px/sec | 10-500 | Running detection sensitivity |
| Loiter time threshold | CONFIG.py | None | 10.0 sec | 1-60 | Loitering detection duration |
| Loiter speed threshold | CONFIG.py | None | 50.0 px/sec | 1-100 | Motion threshold for loitering |
| Fall vertical ratio drop | CONFIG.py | None | 0.4 (40%) | 0.1-0.9 | Fall detection sensitivity |
| Fall downward distance | CONFIG.py | None | 20.0 px | 5-100 | Minimum downward motion for fall |
| Tracker max age | CONFIG.py | None | 30 frames | 5-100 | How long to keep missing tracks |
| Tracker IoU threshold | CONFIG.py | None | 0.3 | 0.1-0.9 | Tracking association strictness |
| Detection confidence | CONFIG.py | `--confidence` | 0.5 | 0.0-1.0 | Object detector threshold |
| Violence threshold | `violence_classifier.py` | `--violence-threshold` | 0.5 | 0.0-1.0 | Violence classification threshold |

### FPS Meter Implementation (VERIFIED)
**Source**: `yolo_object_detection/utils.py` lines 10-39

- **Algorithm**: Exponential Moving Average (EMA)
- **Smoothing factor**: 0.9 (hard-coded)
- **Display**: Top-left corner, green text "FPS: XX.X"
- **Storage**: ‚ùå **NOT LOGGED** - display-only, no file output

---

## 10. Limitations (AUDITED, HONEST)

### Design Scope: Confined Closed Spaces Only
- **Primary use case**: Offices, shops, restricted entry points, security checkpoints
- **Optimal range**: 1-10 people in frame
- **Acceptable range**: 10-15 people (not tested, estimated)
- **NOT suitable for**:
  - ‚ùå Large crowds (50+ people)
  - ‚ùå Outdoor scenes
  - ‚ùå Shopping malls or train stations
  - ‚ùå Concert venues or stadiums

### Violence Classification Accuracy Degradation
- **1-10 people**: ~97.7% validation accuracy (controlled dataset)
- **Real-world confined spaces**: **NOT MEASURED** (unverified ~70% estimate)
- **10-15 people**: Performance unknown (estimate: ~60%, not tested)
- **50+ people**: Performance very poor (estimate: <50%, not recommended)

### Behavior Detection Limitations
- **RUN detection**: Speed threshold calibrated for indoor spaces, may miss running in large open areas
- **FALL detection**: Heuristic-based (aspect ratio + motion), may have false positives on sudden movements
- **LOITER detection**: Requires zone definitions, may not work without configured zones
- **Tracking stability**: Degrades significantly with >15 simultaneous people (estimated, not tested)
- **No re-identification**: People who disappear and reappear get new track IDs (no person re-ID model)

### General Limitations
- **Single-stream only**: Cannot monitor multiple cameras simultaneously
- **No real-time alerting**: Events logged to CSV, no webhook/email/SMS integration
- **Storage unbounded**: Screenshot/event log storage grows without limits
- **No GPU fallback**: Assumes GPU available (will fall back to CPU if missing, but slowly)
- **RTSP NOT validated**: `cv2.VideoCapture` accepts RTSP URLs but no reconnection handling, no network error recovery
- **Running speed resolution-dependent**: Threshold (50 px/sec) varies with camera resolution - not normalized
- **FPS display-only**: No performance metrics logged to file (see Section 9)
- **No frame drop detection**: Cannot measure or report frame drops from source
- **No temporal smoothing**: Violence detected per-frame (potential flicker, no averaging)

### Image Mode Constraints
- **Works**: Object detection, weapon detection, violence classification (single-frame analysis)
- **Doesn't work**: RUN, FALL, LOITER (all require motion history)
- **Use case**: Single frame analysis or post-processing, not real-time monitoring

---

## 11. How to Reproduce / Verify Everything (COMMANDS)

### Verify 97.7% Accuracy
```bash
# Check source of 97.7% accuracy metric
cat runs/violence_cls/train/results.csv | tail -1
# Expected output (last row): epoch 30 with metrics/accuracy_top1: 0.97723

# Or programmatically:
python -c "import pandas as pd; df = pd.read_csv('runs/violence_cls/train/results.csv'); print(f\"Epoch {int(df.iloc[-1]['epoch'])}: Accuracy = {df.iloc[-1]['metrics/accuracy_top1']:.5f}\")"
```

### Verify Armed-Person Distance Logic
```bash
# Check hard-coded margin value
grep -n "margin = " behaviour_detection/pipeline.py
# Expected: Line 349: margin = 50  # pixels

# Verify exact logic
grep -A 3 "px1 - margin" behaviour_detection/pipeline.py
# Expected: Check if weapon center is inside expanded person bbox with ¬±50px margin
```

### Verify Events CSV Schema
```bash
# Check exact columns
head -1 events.csv
# Expected: timestamp,type,track_id,zone_name,centroid_x,centroid_y

# Or check source code
grep -A 1 "fieldnames=" behaviour_detection/rules.py | grep -A 1 "timestamp"
```

### Verify FPS Meter Implementation
```bash
# Check smoothing factor
grep -n "smoothing_factor" yolo_object_detection/utils.py
# Expected: Line 14: smoothing_factor=0.9

# Check that FPS is display-only
grep -n "fps_log\|fps.*file\|fps.*csv" yolo_object_detection/utils.py
# Expected: No matches (FPS not logged to file)
```

### Measure Real-World Accuracy (IF YOU WANT TO VALIDATE)
```bash
# 1. Prepare ground truth annotations
python -c "
from behaviour_detection.evaluate import EvaluationTool
tool = EvaluationTool()
# Requires manual frame-by-frame annotation in tool
tool.run_manual_annotation('your_test_video.mp4')
# Outputs: ground_truth_*.json with frame-by-frame labels
"

# 2. Run system on same video
python run_behaviour.py --source your_test_video.mp4 \
    --events-csv predictions.csv \
    --violence-model runs/violence_cls/train/weights/best.pt

# 3. Compare predictions vs ground truth
python -c "
from behaviour_detection.evaluate import EvaluationTool
tool = EvaluationTool()
results = tool.run_comparison('your_test_video.mp4', 'ground_truth_*.json', 'predictions.csv')
print(f'Accuracy: {results[\"accuracy\"]:.2%}')
print(f'Precision: {results[\"precision\"]:.2%}')
print(f'Recall: {results[\"recall\"]:.2%}')
"
```

### Verify Single-Image Mode Works
```bash
# Test violence detection on single image
python run_behaviour.py --source test_image.jpg --show \
    --violence-model runs/violence_cls/train/weights/best.pt
# Expected: Shows annotated image with violence detection

# Test that RUN/FALL/LOITER are NOT triggered
# (These require motion history, won't work on single image)
```

### Verify RTSP Limitation (Can't Connect)
```bash
# Attempt RTSP stream (will fail if no reconnection)
python run_behaviour.py --source "rtsp://192.168.1.100:554/stream" --show
# Expected: Either works temporarily or hangs without proper error handling
# Note: No reconnection implemented, will fail on network interruption
```

### Verify Weapon Detection Model
```bash
# Check model location
python -c "
import os
if os.path.exists('runs/weapon_det/weights/best.pt'):
    print('‚úì Weapon detection model found')
else:
    print('‚úó Weapon detection model not found')
"
```

---

## 12. Production Readiness Checklist (WHAT'S MISSING)

The system is **deployment-capable** but NOT **production-ready** without:

### Critical Missing Features
| Feature | Status | Impact | Priority |
|---------|--------|--------|----------|
| Real-world accuracy measurement | ‚ùå Missing | Can't confirm 70% claim | üî¥ CRITICAL |
| Real-time alerting (webhook/email) | ‚ùå Missing | Events only logged, not acted on | üî¥ CRITICAL |
| Multi-stream support | ‚ùå Missing | Can't monitor multiple cameras | üî¥ CRITICAL |
| RTSP reconnection | ‚ùå Missing | Network interruption = crash | üü† HIGH |
| Storage management / retention policy | ‚ùå Missing | Unbounded disk usage | üü† HIGH |
| FPS/latency file logging | ‚ùå Missing | Can't measure deployment performance | üü† HIGH |
| Temporal smoothing for violence | ‚ùå Missing | Per-frame flicker possible | üü† HIGH |
| Frame drop detection | ‚ùå Missing | Can't know if missing events | üü° MEDIUM |
| Automated false positive measurement | ‚ùå Missing | Can't tune thresholds empirically | üü° MEDIUM |
| Network error recovery | ‚ùå Missing | Crash on connection loss | üü° MEDIUM |
| Multi-threading / async processing | ‚ùå Missing | Single-threaded bottleneck | üü° MEDIUM |
| GPU batching for efficiency | ‚ùå Missing | Not optimized for multiple people | üü° MEDIUM |

### Deployment Considerations
- **Alerting**: Integrate with external system (webhook, Kafka, message queue)
- **Scalability**: Add multi-threading or multi-process pool for multiple streams
- **Reliability**: Implement circuit breaker pattern for RTSP reconnection
- **Monitoring**: Add metrics collection (Prometheus, ELK stack)
- **Testing**: Execute `evaluate.py` on real deployment data to confirm accuracy

---

## 13. Glossary

| Term | Definition |
|------|-----------|
| **Track ID** | Unique identifier assigned to a person/object for the duration of tracking |
| **Cooldown** | Minimum time between consecutive events of same type for same person (4 seconds) |
| **Confidence** | YOLOv8 detection confidence (0-1), minimum threshold 0.5 |
| **IoU** | Intersection over Union, measure of overlap between bounding boxes (0-1) |
| **COCO** | Common Objects in Context, dataset of 80 object classes |
| **Centroid** | Center point of bounding box (x, y coordinates) |
| **Aspect ratio** | Width / Height of bounding box |
| **Zone** | Rectangular region of interest defined by coordinates |
| **Smoothing factor** | EMA decay factor (0.9 = 90% previous, 10% new) |
| **Ground truth** | Manually annotated correct labels for evaluation |
| **FPS** | Frames Per Second, processing speed metric |
| **Hard-coded** | Value fixed in source code (requires code change to modify) |
| **Configurable** | Parameter modifiable at runtime via CONFIG.py or CLI |

---

## Final Notes

This documentation reflects the **actual current state of the codebase**, not marketing aspirations. Every claim has been verified against source code or explicitly labeled as **NOT MEASURED** or **NOT VALIDATED**.

### Key Takeaways
1. **97.7% accuracy** = Verified from training logs (controlled dataset only)
2. **Real-world accuracy** = NOT MEASURED (must be validated on deployment data)
3. **Armed person logic** = 50px margin (not 100px)
4. **Violence smoothing** = NONE (per-frame, can flicker)
5. **Image mode** = Only VIOLENCE/DANGER work, RUN/FALL/LOITER require history
6. **Production gaps** = Real-time alerting, multi-stream, RTSP reconnection, storage management

Before deploying to production, address the critical missing features in Section 12.
